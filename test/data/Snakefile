"""
Generates sample maps for test runs
"""

import sys
import subprocess
import os
import pandas as pd
import csv

dataset_name = config['dataset']
location = config['location']  # local, gcs

WARP_EXECUTIONS_BUCKET = (
    'gs://playground-us-central1/cromwell/executions/'
    'WGSMultipleSamplesFromBam'
)
PICARD_SUFFIX_D = {
    'contamination': 'selfSM',
    'alignment_summary_metrics': 'alignment_summary_metrics',
    'duplicate_metrics': 'duplicate_metrics',
    'insert_size_metrics': 'insert_size_metrics',
    'wgs_metrics': 'wgs_metrics',
}

samples_url = (
    f'https://raw.githubusercontent.com/populationgenomics/'
    f'fewgenomes/main/datasets/{dataset_name}/samples.ped'
)
try:
    samples = pd.read_csv(samples_url, sep='\t')
except:
    sys.stderr.write(f'Could not read from {samples_url}')
    raise

# Adding population labels for 2/3 of the samples in a group; the rest will
# be used to test the ancestry inferring methods
SAMPLE_WITH_POP_LABELS = list(samples.groupby(['Population'])\
    .apply(lambda x: x.iloc[:int(x['Population'].size / 1.5)])['Individual.ID'])


rule all:
    input:
        all    = f'sample_maps/{dataset_name}-{location}.csv',
        round1 = f'sample_maps/{dataset_name}-{location}-round1.csv',
        round2 = f'sample_maps/{dataset_name}-{location}-round2.csv',
        toy_regions_bed = 'work/toy-regions.bed'

rule copy_picard_file:
    output:
        'picard_files/{sample}.{suffix}'
    params:
        bucket = WARP_EXECUTIONS_BUCKET,
        fname = lambda wildcards: f'{wildcards.sample}.{wildcards.suffix}'
    shell:
        'gsutil cp "{params.bucket}/**/{params.fname}" '
        'picard_files/ || touch {output}'

def write_sample_maps(rows, hdr, output):
    with open(output.all, 'w', newline='') as out_all, \
            open(output.round1, 'w', newline='') as out_r1, \
            open(output.round2, 'w', newline='') as out_r2:
        csvwriter_all = csv.writer(out_all)
        csvwriter_r1 = csv.writer(out_r1)
        csvwriter_r2 = csv.writer(out_r2)
        csvwriter_all.writerow(hdr)
        csvwriter_r1.writerow(hdr)
        csvwriter_r2.writerow(hdr)
        for row in rows:
            csvwriter_all.writerow(row[h] for h in hdr)
        round1_row_num = int(len(rows) // 1.5)
        for row in rows[:round1_row_num]:
            csvwriter_r1.writerow(row[h] for h in hdr)
        for row in rows[round1_row_num:]:
            csvwriter_r2.writerow(row[h] for h in hdr)

def run_cmd(cmd):
    print(cmd)
    return subprocess.run(cmd, shell=True)

def run_check_output(cmd, silent=False):
    if not silent:
        print(cmd)
    return subprocess.check_output(cmd, shell=True).decode()

rule find_all_gvcf_gcs_files:
    output:
        paths_txt = protected(f'work/gcs-paths/gvcf.txt')
    params:
        bucket = WARP_EXECUTIONS_BUCKET
    run:
        gvcf_ptrn = f'{params.bucket}/**/call-MergeVCFs/*.g.vcf.gz'
        run_cmd(f'gsutil ls \'{gvcf_ptrn}\' > {output.paths_txt}')

rule find_all_picard_gcs_files:
    output:
        paths_txt = protected('work/gcs-paths/{picard_key}.txt')
    params:
        bucket = WARP_EXECUTIONS_BUCKET
    run:
        picard_ptrn = f'{params.bucket}/**/*.{wildcards.picard_key}'
        run_cmd(f'gsutil ls \'{picard_ptrn}\' >> {output.paths_txt}')

rule make_csv_gcs:
    input:
        gvcfs_gcs_paths_txt = rules.find_all_gvcf_gcs_files.output.paths_txt,
        picard_gcs_paths_txts = expand(
            rules.find_all_picard_gcs_files.output.paths_txt,
            picard_key=PICARD_SUFFIX_D.keys()
        )
    output:
        all    = f'sample_maps/{dataset_name}-gcs.csv',
        round1 = f'sample_maps/{dataset_name}-gcs-round1.csv',
        round2 = f'sample_maps/{dataset_name}-gcs-round2.csv',
    run:
        rows = []
        hdr = ['sample', 'population', 'gvcf'] + list(PICARD_SUFFIX_D.keys())
        for sample, pop in zip(
                samples['Individual.ID'],
                samples['Population'],
            ):
            try:
                gvcf_path = run_check_output(
                    f'grep {sample}.g.vcf.gz {input.gvcfs_gcs_paths_txt} | head -n1',
                    silent=True
                ).strip()
            except:
                sys.stderr.write(f'{sample}.g.vcf.gz not found')
                continue
            if gvcf_path == '':
                sys.stderr.write(f'GVCF for {sample} is empty')
                continue
            row = dict(sample=sample, gvcf=gvcf_path,
                population=pop if sample in SAMPLE_WITH_POP_LABELS else '')

            for picard_key, picard_suffix in PICARD_SUFFIX_D.items():
                gcs_paths_txt = rules.find_all_picard_gcs_files.output.paths_txt\
                    .replace('{picard_key}', picard_key)
                picard_path = run_check_output(
                    f'grep {sample}.{picard_suffix} {gcs_paths_txt} | head -n1',
                    silent=True
                ).strip()
                row[picard_key] = ''
                if picard_path.strip() != '':
                    row[picard_key] = picard_path
            rows.append(row)
        assert rows[0]['gvcf'] != ''
        write_sample_maps(rows, hdr, output)

rule make_csv_local:
    input:
        gvcfs = expand(rules.clean_info_fields.output, sample=samples['Individual.ID']),
        picard_files = expand(
            rules.copy_picard_file.output,
            sample=samples['Individual.ID'],
            suffix=PICARD_SUFFIX_D.values(),
        )
    output:
        all    = f'sample_maps/{dataset_name}-local.csv',
        round1 = f'sample_maps/{dataset_name}-local-round1.csv',
        round2 = f'sample_maps/{dataset_name}-local-round2.csv',
    run:
        rows = []
        hdr = ['sample', 'population', 'gvcf'] + list(PICARD_SUFFIX_D.keys())
        for sample, pop, gvcf in zip(
                samples['Individual.ID'],
                samples['Population'],
                input.gvcfs
            ):
            row = dict(sample=sample, gvcf=f'data/{gvcf}',
                population=pop if sample in SAMPLE_WITH_POP_LABELS else '')
            for picard_key, picard_suffix in PICARD_SUFFIX_D.items():
                picard_fpath = f'picard_files/{sample}.{picard_suffix}'
                row[picard_key] = ''
                if os.path.isfile(picard_fpath):
                    if os.path.getsize(picard_fpath) == 0:
                        os.remove(picard_fpath)
                    else:
                        row[picard_key] = f'data/{picard_fpath}'
            rows.append(row)
        write_sample_maps(rows, hdr, output)
