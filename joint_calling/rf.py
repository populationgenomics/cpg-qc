"""
Create jobs to create and apply a Random Forest model
"""

import uuid
from os.path import join
from typing import List, Optional
import logging
import hailtop.batch as hb
from hailtop.batch.job import Job
from analysis_runner import dataproc

from joint_calling import utils

logger = logging.getLogger('joint-calling')
logger.setLevel('INFO')


def make_rf_jobs(
    b: hb.Batch,
    combined_mt_path: str,
    info_split_ht_path: str,
    hard_filtered_samples_ht_path: str,
    meta_ht_path: str,
    work_bucket: str,
    depends_on: Optional[List[Job]],
    scripts_dir: str,
    ped_file: Optional[str],
    overwrite: bool = False,
) -> Job:
    """
    :param b: Batch object to add jobs to
    :param combined_mt_path: path to a Matrix Table combined with the Hail VCF combiner
    :param info_split_ht_path: info table with split multiallelics, generated by generate_info_ht.py --out-split-ht-path
    :param hard_filtered_samples_ht_path: a Table containing only samples that failed the hard filters
    :param meta_ht_path: a Table with meta data from sample_qc.py
    :param work_bucket: bucket for intermediate files
    :param depends_on: job that the created jobs should only run after
    :param scripts_dir: repository directory with scripts
    :return: a Job object of the last created job
    """
    fam_stats_ht_path = join(work_bucket, 'fam_stats.ht')

    rf_bucket = join(work_bucket, 'rf')
    allele_data_ht_path = join(rf_bucket, 'allele_data.ht')
    qc_ac_ht_path = join(rf_bucket, 'qc_ac.ht')

    if overwrite or not all(
        utils.file_exists(fp) for fp in [allele_data_ht_path, qc_ac_ht_path]
    ):
        rf_anno_job = dataproc.hail_dataproc_job(
            b,
            f'{scripts_dir}/generate_rf_annotations.py --overwrite '
            + f'--mt {combined_mt_path} '
            + f'--hard-filtered-samples-ht {hard_filtered_samples_ht_path} '
            + f'--meta-ht {meta_ht_path} '
            + f'--out-allele-data-ht {allele_data_ht_path} '
            + f'--out-qc-ac-ht {qc_ac_ht_path} '
            + f'--out-fam-stats-ht {fam_stats_ht_path} '
            + (f'--fam-file {ped_file} ' if ped_file else '')
            + f'--bucket {rf_bucket} ',
            max_age='8h',
            packages=utils.DATAPROC_PACKAGES,
            num_secondary_workers=10,
            depends_on=depends_on,
            job_name='RF: gen QC anno',
            vep='GRCh38',
        )
    else:
        rf_anno_job = b.new_job('RF: gen QC anno')

    freq_ht_path = join(rf_bucket, 'frequencies.ht')

    if overwrite or not utils.file_exists(freq_ht_path):
        rf_freq_data_job = dataproc.hail_dataproc_job(
            b,
            f'{scripts_dir}/generate_freq_data.py --overwrite '
            f'--mt {combined_mt_path} '
            f'--hard-filtered-samples-ht {hard_filtered_samples_ht_path} '
            f'--meta-ht {meta_ht_path} '
            f'--out-ht {freq_ht_path} '
            f'--bucket {rf_bucket} ',
            max_age='8h',
            packages=utils.DATAPROC_PACKAGES,
            num_secondary_workers=10,
            depends_on=depends_on,
            job_name='RF: gen freq data',
        )
    else:
        rf_freq_data_job = b.new_job('RF: gen freq data')

    rf_annotations_ht_path = join(rf_bucket, 'rf_annotations.ht')
    rf_result_ht_path = join(rf_bucket, 'rf_result.ht')
    rf_model_id = f'rf_{str(uuid.uuid4())[:8]}'

    if overwrite or not any(
        utils.file_exists(path) for path in [rf_annotations_ht_path, rf_result_ht_path]
    ):
        rf_job = dataproc.hail_dataproc_job(
            b,
            f'{scripts_dir}/random_forest.py --overwrite '
            f'--info-split-ht {info_split_ht_path} '
            f'--freq-ht {freq_ht_path} '
            f'--allele-data-ht {allele_data_ht_path} '
            f'--qc-ac-ht {qc_ac_ht_path} '
            f'--bucket {rf_bucket} '
            f'--use-adj-genotypes '
            f'--out-model-id {rf_model_id} '
            f'--out-annotations-ht {rf_annotations_ht_path} '
            f'--out-ht {rf_result_ht_path} ',
            max_age='8h',
            packages=utils.DATAPROC_PACKAGES,
            num_secondary_workers=10,
            depends_on=[rf_freq_data_job, rf_anno_job],
            job_name='RF: run',
        )
    else:
        rf_job = b.new_job('RF: run')

    score_bin_ht_path = join(rf_bucket, 'rf_score_bin.ht')
    score_bin_agg_ht_path = join(rf_bucket, 'rf_score_agg_bin.ht')
    if overwrite or not utils.file_exists(score_bin_ht_path):
        eval_job = dataproc.hail_dataproc_job(
            b,
            f'{scripts_dir}/evaluation.py --overwrite '
            f'--info-split-ht {info_split_ht_path} '
            f'--rf-results-ht {rf_result_ht_path} '
            f'--rf-annotations-ht {rf_annotations_ht_path} '
            f'--fam-stats-ht {fam_stats_ht_path} '
            f'--mt {combined_mt_path} '
            f'--bucket {work_bucket} '
            f'--out-bin-ht {score_bin_ht_path} '
            f'--out-aggregated-bin-ht {score_bin_agg_ht_path} '
            f'--run-sanity-checks ',
            max_age='8h',
            packages=utils.DATAPROC_PACKAGES,
            num_secondary_workers=10,
            depends_on=[rf_job],
            job_name='RF: evaluation',
        )

    else:
        eval_job = b.new_job('RF: evaluation')

    return eval_job
