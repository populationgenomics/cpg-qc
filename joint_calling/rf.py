"""
Create jobs to create and apply a Random Forest model
"""

import uuid
from os.path import join
from typing import List, Optional
import logging
import hailtop.batch as hb
from hailtop.batch.job import Job
from analysis_runner import dataproc

from joint_calling import utils

logger = logging.getLogger('joint-calling')
logger.setLevel('INFO')


def make_rf_jobs(
    b: hb.Batch,
    combined_mt_path: str,
    info_split_ht_path: str,
    hard_filtered_samples_ht_path: str,
    meta_ht_path: str,
    work_bucket: str,
    depends_on: Optional[List[Job]],
    scripts_dir: str,
    ped_file: Optional[str],
    overwrite: bool = False,
) -> Job:
    """
    Create Batch jobs that run the random forest variant QC

    :param b: Batch object to add jobs to
    :param combined_mt_path: path to a Matrix Table combined with the Hail VCF combiner
    :param info_split_ht_path: info table with split multiallelics, generated by generate_info_ht.py --out-split-ht-path
    :param hard_filtered_samples_ht_path: a Table containing only samples that failed the hard filters
    :param meta_ht_path: a Table with meta data from sample_qc.py
    :param work_bucket: bucket for intermediate files
    :param depends_on: job that the created jobs should only run after
    :param scripts_dir: repository directory with scripts
    :param ped_file: pedigree file for the samples
    :param overwrite: whether to overwrite existing intermediate and output files
    :return: a Job object of the last created job
    """
    fam_stats_ht_path = join(work_bucket, 'fam-stats.ht')
    allele_data_ht_path = join(work_bucket, 'allele-data.ht')
    qc_ac_ht_path = join(work_bucket, 'qc-ac.ht')

    if overwrite or not all(
        utils.file_exists(fp) for fp in [allele_data_ht_path, qc_ac_ht_path]
    ):
        rf_anno_job = dataproc.hail_dataproc_job(
            b,
            f'{scripts_dir}/generate_rf_annotations.py --overwrite '
            + f'--mt {combined_mt_path} '
            + f'--hard-filtered-samples-ht {hard_filtered_samples_ht_path} '
            + f'--meta-ht {meta_ht_path} '
            + f'--out-allele-data-ht {allele_data_ht_path} '
            + f'--out-qc-ac-ht {qc_ac_ht_path} '
            + f'--out-fam-stats-ht {fam_stats_ht_path} '
            + (f'--fam-file {ped_file} ' if ped_file else '')
            + f'--bucket {work_bucket} ',
            max_age='8h',
            packages=utils.DATAPROC_PACKAGES,
            num_secondary_workers=10,
            depends_on=depends_on,
            job_name='RF: gen QC anno',
            vep='GRCh38',
        )
    else:
        rf_anno_job = b.new_job('RF: gen QC anno')

    freq_ht_path = join(work_bucket, 'frequencies.ht')
    if overwrite or not utils.file_exists(freq_ht_path):
        rf_freq_data_job = dataproc.hail_dataproc_job(
            b,
            f'{scripts_dir}/generate_freq_data.py --overwrite '
            f'--mt {combined_mt_path} '
            f'--hard-filtered-samples-ht {hard_filtered_samples_ht_path} '
            f'--meta-ht {meta_ht_path} '
            f'--out-ht {freq_ht_path} '
            f'--bucket {work_bucket} ',
            max_age='8h',
            packages=utils.DATAPROC_PACKAGES,
            num_secondary_workers=10,
            depends_on=depends_on,
            job_name='RF: gen freq data',
        )
    else:
        rf_freq_data_job = b.new_job('RF: gen freq data')

    rf_annotations_ht_path = join(work_bucket, 'rf-annotations.ht')
    rf_result_ht_path = join(work_bucket, 'rf-result.ht')
    rf_model_id = f'rf_{str(uuid.uuid4())[:8]}'
    if overwrite or not any(
        utils.file_exists(path) for path in [rf_annotations_ht_path, rf_result_ht_path]
    ):
        rf_job = dataproc.hail_dataproc_job(
            b,
            f'{scripts_dir}/random_forest.py --overwrite '
            f'--info-split-ht {info_split_ht_path} '
            f'--freq-ht {freq_ht_path} '
            f'--allele-data-ht {allele_data_ht_path} '
            f'--qc-ac-ht {qc_ac_ht_path} '
            f'--bucket {work_bucket} '
            f'--use-adj-genotypes '
            f'--out-model-id {rf_model_id} '
            f'--out-annotations-ht {rf_annotations_ht_path} '
            f'--out-ht {rf_result_ht_path} ',
            max_age='8h',
            packages=utils.DATAPROC_PACKAGES,
            num_secondary_workers=10,
            depends_on=[rf_freq_data_job, rf_anno_job],
            job_name='RF: run',
        )
    else:
        rf_job = b.new_job('RF: run')

    return (
        rf_job,
        info_split_ht_path,
        rf_model_id,
        rf_annotations_ht_path,
        rf_result_ht_path,
        fam_stats_ht_path,
        freq_ht_path,
    )


def make_rf_eval_jobs(
    b: hb.Batch,
    combined_mt_path: str,
    info_split_ht_path: str,
    rf_result_ht_path: str,
    rf_annotations_ht_path: str,
    fam_stats_ht_path: str,
    freq_ht_path: str,
    rf_model_id: str,
    work_bucket: str,
    overwrite: bool,
    scripts_dir: str,
    depends_on: Optional[List[Job]],
) -> Job:
    """
    Make jobs that do evaluation RF model and applies the final filters
    """
    score_bin_ht_path = join(work_bucket, 'rf-score-bin.ht')
    score_bin_agg_ht_path = join(work_bucket, 'rf-score-agg-bin.ht')
    if overwrite or not utils.file_exists(score_bin_ht_path):
        eval_job = dataproc.hail_dataproc_job(
            b,
            f'{scripts_dir}/evaluation.py --overwrite '
            f'--info-split-ht {info_split_ht_path} '
            f'--rf-results-ht {rf_result_ht_path} '
            f'--rf-annotations-ht {rf_annotations_ht_path} '
            f'--fam-stats-ht {fam_stats_ht_path} '
            f'--mt {combined_mt_path} '
            f'--bucket {work_bucket} '
            f'--out-bin-ht {score_bin_ht_path} '
            f'--out-aggregated-bin-ht {score_bin_agg_ht_path} '
            f'--run-sanity-checks ',
            max_age='8h',
            packages=utils.DATAPROC_PACKAGES,
            num_secondary_workers=10,
            depends_on=depends_on,
            job_name='RF: evaluation',
        )
    else:
        eval_job = b.new_job('RF: evaluation')

    final_filter_ht_path = join(work_bucket, 'final-filter.ht')
    if not utils.file_exists(final_filter_ht_path):
        final_filter_job = dataproc.hail_dataproc_job(
            b,
            f'{scripts_dir}/final_filter.py --overwrite '
            f'--out-final-filter-ht {final_filter_ht_path} '
            f'--model-id {rf_model_id} '
            f'--model-name RF '
            f'--score-name RF '
            f'--info-split-ht {info_split_ht_path} '
            f'--freq-ht {freq_ht_path} '
            f'--score-bin-ht {score_bin_ht_path} '
            f'--score-bin-agg-ht {score_bin_agg_ht_path} ' + f'--bucket {work_bucket} ',
            max_age='8h',
            packages=utils.DATAPROC_PACKAGES,
            num_secondary_workers=10,
            depends_on=[eval_job],
            job_name='RF: final filter',
        )
    else:
        final_filter_job = b.new_job('RF: final filter')

    return (
        final_filter_job,
        info_split_ht_path,
        rf_model_id,
        rf_annotations_ht_path,
        rf_result_ht_path,
        fam_stats_ht_path,
        freq_ht_path,
    )
